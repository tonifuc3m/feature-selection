{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Antonio Miranda Escalada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SOME PRELIMINARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\toni3\\\\Documents\\\\Universidad\\\\Master\\\\2\\\\BD Intelligence methods and technologies\\\\Assignments\\\\3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "# For Panda dataframes. A dataframe is a matrix-like structure, \n",
    "# similar to R dataframes  \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics, neighbors, tree\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import mlxtend\n",
    "from kneed import KneeLocator\n",
    "\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including \n",
    "# the outcome to be predicted)\n",
    "print(data.shape)\n",
    "#data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below, data is going to be separated in train, validation, and test. Given that the use of Pandas dataframes is quite advanced, I am doing this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indicesTrain = (np.where(data.year<=2006))[0]\n",
    "indicesVal = (np.where((data.year==2007) | (data.year==2008)))[0]\n",
    "indicesTest = (np.where(data.year>=2009))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware!, **indicesTrain** does not contain the training data, but the *indices* of the training data. For instance, the following cell means that training data is made of instance number 0, instance number 1, ..., up to instance number 2527. This will be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2525, 2526, 2527], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicesTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to transform **data**, which is a Pandas dataframe, to **ava**, which is a NumPy matrix. The reason is that Scikit-learn uses NumPy matrices, not Panda dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **ava** is going to be decomposed into inputs **X** and outputs **y**. And then, into training, validation, and test. For instance, **Xava** and **yava** contain the input attributes, and the output attribute (**energy**) of the whole dataset. Please, ask yourself why the inputs use \"6:\" and the output use \"0\". **Xtrain** and **ytrain** are the same, but for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xava = ava[:,6:]; yava = ava[:,0]\n",
    "Xtrain = ava[indicesTrain,6:]; ytrain = ava[indicesTrain,0]\n",
    "Xval = ava[indicesVal,6:]; yval = ava[indicesVal,0]\n",
    "Xtest = ava[indicesTest,6:]; ytest = ava[indicesTest,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataset with train and validation data\n",
    "indicesTrainAndVal = (np.where(data.year<=2008))[0]\n",
    "XtrainAndVal = ava[indicesTrainAndVal,6:]; ytrainAndVal = ava[indicesTrainAndVal,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines function **mae** (Mean Absolute Error), that we will use later to measure the accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(yval_pred, yval):\n",
    "  val_mae = metrics.mean_absolute_error(yval_pred, yval)\n",
    "  return(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains KNN with (Xtrain, ytrain) and evaluates it with (Xval, yval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 151 ms\n",
      "MAE for KNN with K=5 is 486.91141493456513\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "%time _ = knn.fit(Xtrain, ytrain)\n",
    "yval_pred = knn.predict(Xval)\n",
    "\n",
    "print(\"MAE for KNN with K=5 is {}\".format(mae(yval_pred, yval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in sklearn.neighbors:\n",
      "\n",
      "sklearn.neighbors.KNeighborsRegressor = class KNeighborsRegressor(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.SupervisedFloatMixin, sklearn.base.RegressorMixin)\n",
      " |  sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |  \n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : str or callable\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : integer, optional (default = 2)\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : string or callable, default 'minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of the DistanceMetric class for a\n",
      " |      list of available metrics.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [ 0.5]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  NearestNeighbors\n",
      " |  RadiusNeighborsRegressor\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.SupervisedFloatMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of int, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Target values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.SupervisedFloatMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values, array of float values, shape = [n_samples]\n",
      " |           or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In case you need help for KNN\n",
    "help('sklearn.neighbors.KNeighborsRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell, does hyper-parameter tuning for parameter K (n_neighbors), from 1 to 4 by 1. Please, notice that with **partitions = [(indicesTrain, indicesVal)]** we are telling **gridSearch** to use the training dataset for training the different models with the different parameters, and the validation dataset for testing. Notice that this is different to other notebooks, where crossvalidation was used for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "param_grid = {'n_neighbors': list(range(1,4,1))}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "\n",
    "clf = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_absolute_error',\n",
    "                   cv=partitions , verbose=0)\n",
    "%time _ = clf.fit(Xava,yava)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the best K parameter and the MAE of the final model built with the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: {'n_neighbors': 3} and MAE for best K: 503.71169104439315\n"
     ]
    }
   ],
   "source": [
    "print(\"Best K: {} and MAE for best K: {}\".format(clf.best_params_, -clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HOW LONG DOES IT TAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to have some estimation of how long your machine learning algorithm is going to take. In the next two cells, try to estimate how many seconds KNN (with K=3) does it take, with only **100 instances**. With 6000 instances, it will take approximately 60 times that number. You can use **%time** for timing, as in previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.01 ms\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_neighbors = 3\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "%time _ = knn.fit(Xtrain[0:100,], ytrain[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do the same for Decision trees with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.1 ms\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "method_tree = tree.DecisionTreeRegressor()\n",
    "%time _ = method_tree.fit(Xtrain[0:100,], ytrain[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN model with default hiper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for KNN default is 486.91141493456513\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "model_knn = knn.fit(Xtrain, ytrain)\n",
    "knn_pred = model_knn.predict(Xval)\n",
    "print(\"MAE for KNN default is {}\".format(mae(knn_pred, yval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for KNN. Can you improve results? Note: I will specially value if you use model based optimization (**skopt**). If not possible, use **Randomized Search** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bayes search K: {'n_neighbors': 17} and MAE for best K: 469.78749943395366\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning --> Bayesian Optimization\n",
    "np.random.seed(0)\n",
    "\n",
    "param_knn_bayes_grid = {'n_neighbors': (1,20)}\n",
    "budget = 10\n",
    "method_knn_bayes = BayesSearchCV(\n",
    "        neighbors.KNeighborsRegressor(), \n",
    "        param_knn_bayes_grid, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        refit = False,\n",
    "        cv = partitions, \n",
    "        n_jobs =1, \n",
    "        verbose = 0,\n",
    "        n_iter = budget\n",
    ")\n",
    "\n",
    "model_knn_bayes = method_knn_bayes.fit(Xava, yava)\n",
    "print(\"Best Bayes search K: {} and MAE for best K: {}\".format(model_knn_bayes.best_params_, \n",
    "                                                              -model_knn_bayes.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for KNN final is 511.40569863395604\n"
     ]
    }
   ],
   "source": [
    "#The final knn model is going to be trained with train and validation\n",
    "#datasets in order to obtain the final performance over the test partition\n",
    "np.random.seed(0)\n",
    "\n",
    "knn_final = neighbors.KNeighborsRegressor()\n",
    "knn_final.set_params(**model_knn_bayes.best_params_)\n",
    "model_knn_final = knn_final.fit(XtrainAndVal, ytrainAndVal)\n",
    "model_knn_final_test_pred = model_knn_final.predict(Xtest)\n",
    "print(\"MAE for KNN final is {}\".format(mae(model_knn_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree for regression with default hiper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for DecisionTree default is 371.82943033102384\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "method_tree = tree.DecisionTreeRegressor()\n",
    "model_tree = method_tree.fit(Xtrain, ytrain)\n",
    "tree_pred = model_tree.predict(Xval)\n",
    "print(\"MAE for DecisionTree default is {}\".format(mae(tree_pred, yval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Decision trees. Can you improve results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bayes search max_depth, min_samples_split: {'max_depth': 6, 'min_samples_split': 81} and MAE for best Decision tree: 303.7726118568905\n",
      "MAE for Decision Tree final is 310.9637342803766\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning --> Bayesian Optimization\n",
    "np.random.seed(0)\n",
    "\n",
    "#In spite of the different hyper-parameters for a decision tree (max_depth, min_samples_split, max_features, min_samples_leaf),\n",
    "#only max_depth and min_samples_split are used because of the academic goal of this study\n",
    "param_tree_bayes_grid = {'max_depth': (2,30),\n",
    "                         'min_samples_split': (2,100)\n",
    "}\n",
    "budget = 40\n",
    "method_tree_bayes = BayesSearchCV(\n",
    "        tree.DecisionTreeRegressor(), \n",
    "        param_tree_bayes_grid, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        refit = False,\n",
    "        cv = partitions, \n",
    "        n_jobs =1, \n",
    "        verbose = 0,\n",
    "        n_iter = budget\n",
    ")\n",
    "\n",
    "model_tree_bayes = method_tree_bayes.fit(Xava, yava)\n",
    "print(\"Best Bayes search max_depth, min_samples_split: {} \\\n",
    "and MAE for best Decision tree: {}\".format(model_tree_bayes.best_params_, -model_tree_bayes.best_score_))\n",
    "\n",
    "tree_final = tree.DecisionTreeRegressor()\n",
    "tree_final.set_params(**model_tree_bayes.best_params_)\n",
    "model_tree_final = tree_final.fit(XtrainAndVal, ytrainAndVal)\n",
    "model_tree_final_test_pred = model_tree_final.predict(Xtest)\n",
    "print(\"MAE for Decision Tree final is {}\".format(mae(model_tree_final_test_pred, ytest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest (RF) with default parameters. A RF is an ensemble technique based on Decision Trees, but instead of training just a single decision tree, it trains many of them and then computes the average of the outputs. Please, bear in mind that a RF with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Random Forest default is 287.27191993841416\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "model_rf = rf.fit(Xtrain, ytrain)\n",
    "rf_pred = model_rf.predict(Xval)\n",
    "print(\"MAE for Random Forest default is {}\".format(mae(rf_pred, yval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Random Forests. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bayes search n_estimator: {'n_estimators': 200} and MAE for best Random Forest: 274.8996147806004\n",
      "MAE for Random Forest final is 283.42557500000004\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning --> Bayesian Optimization\n",
    "np.random.seed(0)\n",
    "\n",
    "param_rf_bayes_grid = {'n_estimators': (5,200)\n",
    "}\n",
    "budget = 40\n",
    "rf_bayes = BayesSearchCV(\n",
    "        RandomForestRegressor(), \n",
    "        param_rf_bayes_grid, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        refit = False,\n",
    "        cv = partitions, \n",
    "        n_jobs =1, \n",
    "        verbose = 0,\n",
    "        n_iter = budget\n",
    ")\n",
    "\n",
    "model_rf_bayes = rf_bayes.fit(Xava, yava)\n",
    "print(\"Best Bayes search n_estimator: {} and MAE for best Random Forest: {}\".format(model_rf_bayes.best_params_,\n",
    "                                                                                    -model_rf_bayes.best_score_))\n",
    "\n",
    "rf_final = RandomForestRegressor()\n",
    "rf_final.set_params(**model_rf_bayes.best_params_)\n",
    "model_rf_final = rf_final.fit(XtrainAndVal, ytrainAndVal)\n",
    "model_rf_final_test_pred = model_rf_final.predict(Xtest)\n",
    "print(\"MAE for Random Forest final is {}\".format(mae(model_rf_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Gradient Tree Boosting (GB) with default parameters. A GB is also an ensemble technique based on Decision Trees. In this case, the second decision tree tries to fix the mistakes of the first decision tree. The third decision tree tries to fix the mistakes of the first two decision trees. An so on.\n",
    "\n",
    "Please, bear in mind that a GB with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Gradient Boosting default is 280.82732452932765\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "model_gb = gb.fit(Xtrain, ytrain)\n",
    "gb_pred = model_gb.predict(Xval)\n",
    "print(\"MAE for Gradient Boosting default is {}\".format(mae(gb_pred, yval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Gradient Boosting. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bayes search n_estimator: {'n_estimators': 133} and MAE for best Gradient Boosting: 278.5243999140768\n",
      "MAE for Gradient Boosting final is 289.59716170662733\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "param_gb_bayes_grid = {'n_estimators': (50,200)\n",
    "}\n",
    "budget = 40\n",
    "method_gb_bayes = BayesSearchCV(\n",
    "        GradientBoostingRegressor(), \n",
    "        param_gb_bayes_grid, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        refit = False,\n",
    "        cv = partitions, \n",
    "        n_jobs =1, \n",
    "        verbose = 0,\n",
    "        n_iter = budget\n",
    ")\n",
    "\n",
    "model_gb_bayes = method_gb_bayes.fit(Xava, yava)\n",
    "print(\"Best Bayes search n_estimator: {} and MAE for best Gradient Boosting: {}\".format(model_gb_bayes.best_params_,\n",
    "                                                                                        -model_gb_bayes.best_score_))\n",
    "\n",
    "gb_final = GradientBoostingRegressor()\n",
    "gb_final.set_params(**model_gb_bayes.best_params_)\n",
    "model_gb_final = gb_final.fit(XtrainAndVal, ytrainAndVal)\n",
    "model_gb_final_test_pred = model_gb_final.predict(Xtest)\n",
    "print(\"MAE for Gradient Boosting final is {}\".format(mae(model_gb_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try now with a more powerful implementation of GB: CatBoost (this implementation is specific for categorical data, but the library XGBoost did not work properly on our personal laptops, and CatBoost also performs well on numeric data). Due to the computational time of CatBoost we had to use a very low number of estimators. Then, the final performance is poor. However, we know it is powerful and should compete with Random Forest and the scikit implementation of Gradient Boosting Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 772ms\tremaining: 6.95s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.23s\tremaining: 4.9s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.65s\tremaining: 3.85s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.1s\tremaining: 3.15s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.52s\tremaining: 2.52s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.97s\tremaining: 1.98s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.39s\tremaining: 1.45s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.83s\tremaining: 958ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.28s\tremaining: 475ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.73s\tremaining: 0us\n",
      "MAE forXGBoost default is 563.3728300381824\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostRegressor(iterations=10)\n",
    "model.fit(Xtrain, ytrain)\n",
    "preds = model.predict(Xval)\n",
    "print(\"MAE forXGBoost default is {}\".format(mae(preds, yval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 595ms\tremaining: 2.97s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.03s\tremaining: 2.07s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.47s\tremaining: 1.47s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.91s\tremaining: 957ms\n",
      "4:\tlearn: 848.7492804\ttotal: 2.35s\tremaining: 469ms\n",
      "5:\tlearn: 830.6348224\ttotal: 2.79s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 560ms\tremaining: 1.12s\n",
      "1:\tlearn: 906.3758016\ttotal: 996ms\tremaining: 498ms\n",
      "2:\tlearn: 886.3932667\ttotal: 1.41s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 573ms\tremaining: 2.87s\n",
      "1:\tlearn: 906.3758016\ttotal: 998ms\tremaining: 2s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.42s\tremaining: 1.42s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.86s\tremaining: 929ms\n",
      "4:\tlearn: 848.7492804\ttotal: 2.28s\tremaining: 455ms\n",
      "5:\tlearn: 830.6348224\ttotal: 2.76s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 570ms\tremaining: 570ms\n",
      "1:\tlearn: 906.3758016\ttotal: 1s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 608ms\tremaining: 1.22s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.05s\tremaining: 524ms\n",
      "2:\tlearn: 886.3932667\ttotal: 1.54s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 554ms\tremaining: 4.43s\n",
      "1:\tlearn: 906.3758016\ttotal: 989ms\tremaining: 3.46s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.41s\tremaining: 2.83s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.87s\tremaining: 2.34s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.3s\tremaining: 1.84s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.75s\tremaining: 1.38s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.17s\tremaining: 907ms\n",
      "7:\tlearn: 796.6452840\ttotal: 3.61s\tremaining: 451ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.06s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 549ms\tremaining: 4.94s\n",
      "1:\tlearn: 906.3758016\ttotal: 981ms\tremaining: 3.92s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.42s\tremaining: 3.32s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.88s\tremaining: 2.82s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.31s\tremaining: 2.31s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.77s\tremaining: 1.84s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.23s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.71s\tremaining: 928ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.15s\tremaining: 461ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.59s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 553ms\tremaining: 2.77s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.02s\tremaining: 2.05s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.43s\tremaining: 1.43s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.86s\tremaining: 931ms\n",
      "4:\tlearn: 848.7492804\ttotal: 2.29s\tremaining: 459ms\n",
      "5:\tlearn: 830.6348224\ttotal: 2.75s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 557ms\tremaining: 557ms\n",
      "1:\tlearn: 906.3758016\ttotal: 971ms\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 548ms\tremaining: 1.64s\n",
      "1:\tlearn: 906.3758016\ttotal: 972ms\tremaining: 972ms\n",
      "2:\tlearn: 886.3932667\ttotal: 1.39s\tremaining: 464ms\n",
      "3:\tlearn: 867.4944761\ttotal: 1.83s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 579ms\tremaining: 5.21s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.01s\tremaining: 4.05s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.44s\tremaining: 3.36s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.88s\tremaining: 2.82s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.3s\tremaining: 2.3s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.8s\tremaining: 1.87s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.23s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.68s\tremaining: 919ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.1s\tremaining: 455ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.59s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 605ms\tremaining: 5.45s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.1s\tremaining: 4.4s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.63s\tremaining: 3.79s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.13s\tremaining: 3.19s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.62s\tremaining: 2.62s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.13s\tremaining: 2.09s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.57s\tremaining: 1.53s\n",
      "7:\tlearn: 796.6452840\ttotal: 4.03s\tremaining: 1.01s\n",
      "8:\tlearn: 780.2373449\ttotal: 4.43s\tremaining: 492ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.86s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 558ms\tremaining: 5.02s\n",
      "1:\tlearn: 906.3758016\ttotal: 988ms\tremaining: 3.95s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.45s\tremaining: 3.38s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.95s\tremaining: 2.92s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.42s\tremaining: 2.42s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.9s\tremaining: 1.94s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.39s\tremaining: 1.45s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.88s\tremaining: 971ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.32s\tremaining: 480ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.77s\tremaining: 0us\n",
      "0:\tlearn: 926.2647360\ttotal: 561ms\tremaining: 3.93s\n",
      "1:\tlearn: 906.3758016\ttotal: 999ms\tremaining: 3s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.44s\tremaining: 2.39s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.9s\tremaining: 1.9s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.34s\tremaining: 1.4s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.8s\tremaining: 933ms\n",
      "6:\tlearn: 813.3819569\ttotal: 3.23s\tremaining: 461ms\n",
      "7:\tlearn: 796.6452840\ttotal: 3.68s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 548ms\tremaining: 4.93s\n",
      "1:\tlearn: 906.3758016\ttotal: 969ms\tremaining: 3.87s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.39s\tremaining: 3.24s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.87s\tremaining: 2.81s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.31s\tremaining: 2.31s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.8s\tremaining: 1.86s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.22s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.65s\tremaining: 914ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.07s\tremaining: 452ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.5s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 606ms\tremaining: 5.46s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.07s\tremaining: 4.28s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.54s\tremaining: 3.59s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.01s\tremaining: 3.02s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.44s\tremaining: 2.44s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.91s\tremaining: 1.94s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.34s\tremaining: 1.43s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.78s\tremaining: 944ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.19s\tremaining: 465ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.62s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 609ms\tremaining: 5.48s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.05s\tremaining: 4.21s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.49s\tremaining: 3.47s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.92s\tremaining: 2.88s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.34s\tremaining: 2.34s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.78s\tremaining: 1.85s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.23s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.71s\tremaining: 928ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.18s\tremaining: 465ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.63s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 591ms\tremaining: 5.32s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.05s\tremaining: 4.21s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.53s\tremaining: 3.57s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.01s\tremaining: 3.02s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.48s\tremaining: 2.48s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.94s\tremaining: 1.96s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.39s\tremaining: 1.45s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.84s\tremaining: 961ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.26s\tremaining: 473ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.71s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 547ms\tremaining: 4.92s\n",
      "1:\tlearn: 906.3758016\ttotal: 966ms\tremaining: 3.87s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.39s\tremaining: 3.23s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.88s\tremaining: 2.83s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.3s\tremaining: 2.3s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.73s\tremaining: 1.82s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.16s\tremaining: 1.35s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.59s\tremaining: 899ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.01s\tremaining: 445ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.44s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 615ms\tremaining: 5.54s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.09s\tremaining: 4.34s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.51s\tremaining: 3.53s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.97s\tremaining: 2.95s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.39s\tremaining: 2.39s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.88s\tremaining: 1.92s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.38s\tremaining: 1.45s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.9s\tremaining: 974ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.36s\tremaining: 484ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.84s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 559ms\tremaining: 5.03s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.01s\tremaining: 4.05s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.45s\tremaining: 3.39s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.91s\tremaining: 2.86s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.33s\tremaining: 2.33s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.78s\tremaining: 1.85s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.21s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.65s\tremaining: 913ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.08s\tremaining: 453ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.51s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 548ms\tremaining: 4.93s\n",
      "1:\tlearn: 906.3758016\ttotal: 968ms\tremaining: 3.87s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.38s\tremaining: 3.23s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.83s\tremaining: 2.74s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.25s\tremaining: 2.25s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.69s\tremaining: 1.79s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.11s\tremaining: 1.33s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.55s\tremaining: 887ms\n",
      "8:\tlearn: 780.2373449\ttotal: 3.96s\tremaining: 440ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.38s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 543ms\tremaining: 4.89s\n",
      "1:\tlearn: 906.3758016\ttotal: 962ms\tremaining: 3.85s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.35s\tremaining: 3.14s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.82s\tremaining: 2.73s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.28s\tremaining: 2.28s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.76s\tremaining: 1.84s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.22s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.68s\tremaining: 921ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.13s\tremaining: 459ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.6s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 557ms\tremaining: 5.01s\n",
      "1:\tlearn: 906.3758016\ttotal: 981ms\tremaining: 3.92s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.4s\tremaining: 3.28s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.84s\tremaining: 2.75s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.25s\tremaining: 2.25s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.69s\tremaining: 1.79s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.11s\tremaining: 1.33s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.54s\tremaining: 884ms\n",
      "8:\tlearn: 780.2373449\ttotal: 3.95s\tremaining: 438ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.39s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 584ms\tremaining: 5.26s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.04s\tremaining: 4.15s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.51s\tremaining: 3.53s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.03s\tremaining: 3.05s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.44s\tremaining: 2.44s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.94s\tremaining: 1.96s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.4s\tremaining: 1.46s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.83s\tremaining: 958ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.25s\tremaining: 472ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.7s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 545ms\tremaining: 4.91s\n",
      "1:\tlearn: 906.3758016\ttotal: 968ms\tremaining: 3.87s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.4s\tremaining: 3.27s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.84s\tremaining: 2.77s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.28s\tremaining: 2.28s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.73s\tremaining: 1.82s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.14s\tremaining: 1.35s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.7s\tremaining: 925ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.22s\tremaining: 469ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.72s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 639ms\tremaining: 5.75s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.14s\tremaining: 4.57s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.61s\tremaining: 3.76s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.07s\tremaining: 3.11s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.54s\tremaining: 2.54s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.03s\tremaining: 2.02s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.46s\tremaining: 1.48s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.97s\tremaining: 994ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.46s\tremaining: 496ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.94s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 663ms\tremaining: 5.96s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.09s\tremaining: 4.37s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.52s\tremaining: 3.56s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.97s\tremaining: 2.95s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.38s\tremaining: 2.38s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.83s\tremaining: 1.89s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.26s\tremaining: 1.4s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.75s\tremaining: 936ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.26s\tremaining: 473ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.89s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 552ms\tremaining: 4.97s\n",
      "1:\tlearn: 906.3758016\ttotal: 968ms\tremaining: 3.87s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.4s\tremaining: 3.26s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.82s\tremaining: 2.74s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.25s\tremaining: 2.25s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.71s\tremaining: 1.81s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.18s\tremaining: 1.36s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.67s\tremaining: 918ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.12s\tremaining: 458ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.59s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 580ms\tremaining: 5.22s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.03s\tremaining: 4.14s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.48s\tremaining: 3.46s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.96s\tremaining: 2.94s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.38s\tremaining: 2.38s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.82s\tremaining: 1.88s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.23s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.66s\tremaining: 914ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.07s\tremaining: 453ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.5s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 530ms\tremaining: 4.77s\n",
      "1:\tlearn: 906.3758016\ttotal: 957ms\tremaining: 3.83s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.38s\tremaining: 3.22s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.86s\tremaining: 2.79s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.3s\tremaining: 2.3s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.84s\tremaining: 1.89s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.27s\tremaining: 1.4s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.78s\tremaining: 945ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.24s\tremaining: 471ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.66s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 491ms\tremaining: 4.42s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.01s\tremaining: 4.06s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.49s\tremaining: 3.48s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.03s\tremaining: 3.05s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.52s\tremaining: 2.52s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.02s\tremaining: 2.01s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.52s\tremaining: 1.51s\n",
      "7:\tlearn: 796.6452840\ttotal: 4.06s\tremaining: 1.01s\n",
      "8:\tlearn: 780.2373449\ttotal: 4.55s\tremaining: 506ms\n",
      "9:\tlearn: 764.5699727\ttotal: 5.08s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 550ms\tremaining: 4.95s\n",
      "1:\tlearn: 906.3758016\ttotal: 998ms\tremaining: 3.99s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.58s\tremaining: 3.68s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.07s\tremaining: 3.1s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.51s\tremaining: 2.51s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.97s\tremaining: 1.98s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.4s\tremaining: 1.46s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.87s\tremaining: 969ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.29s\tremaining: 477ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.74s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 563ms\tremaining: 5.06s\n",
      "1:\tlearn: 906.3758016\ttotal: 984ms\tremaining: 3.94s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.41s\tremaining: 3.29s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.06s\tremaining: 3.09s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.64s\tremaining: 2.64s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.22s\tremaining: 2.15s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.7s\tremaining: 1.58s\n",
      "7:\tlearn: 796.6452840\ttotal: 4.23s\tremaining: 1.06s\n",
      "8:\tlearn: 780.2373449\ttotal: 4.8s\tremaining: 534ms\n",
      "9:\tlearn: 764.5699727\ttotal: 5.3s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 592ms\tremaining: 5.33s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.03s\tremaining: 4.14s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.48s\tremaining: 3.46s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.97s\tremaining: 2.95s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.49s\tremaining: 2.49s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.03s\tremaining: 2.02s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.5s\tremaining: 1.5s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.97s\tremaining: 993ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.4s\tremaining: 489ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.87s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 588ms\tremaining: 5.29s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.03s\tremaining: 4.11s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.52s\tremaining: 3.54s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.08s\tremaining: 3.11s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.63s\tremaining: 2.63s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.1s\tremaining: 2.07s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.53s\tremaining: 1.51s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.97s\tremaining: 992ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.39s\tremaining: 488ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.82s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 484ms\tremaining: 4.36s\n",
      "1:\tlearn: 906.3758016\ttotal: 902ms\tremaining: 3.61s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.33s\tremaining: 3.1s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.82s\tremaining: 2.73s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.23s\tremaining: 2.23s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.79s\tremaining: 1.86s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.21s\tremaining: 1.38s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.65s\tremaining: 912ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.07s\tremaining: 452ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.5s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 814ms\tremaining: 7.33s\n",
      "1:\tlearn: 906.3758016\ttotal: 1.35s\tremaining: 5.42s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.77s\tremaining: 4.14s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.2s\tremaining: 3.31s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.68s\tremaining: 2.68s\n",
      "5:\tlearn: 830.6348224\ttotal: 3.17s\tremaining: 2.11s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.58s\tremaining: 1.53s\n",
      "7:\tlearn: 796.6452840\ttotal: 4.07s\tremaining: 1.02s\n",
      "8:\tlearn: 780.2373449\ttotal: 4.47s\tremaining: 496ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.97s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 544ms\tremaining: 4.9s\n",
      "1:\tlearn: 906.3758016\ttotal: 1s\tremaining: 4.02s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.6s\tremaining: 3.73s\n",
      "3:\tlearn: 867.4944761\ttotal: 2.1s\tremaining: 3.15s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.52s\tremaining: 2.52s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.95s\tremaining: 1.97s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.4s\tremaining: 1.46s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.89s\tremaining: 973ms\n",
      "8:\tlearn: 780.2373449\ttotal: 4.34s\tremaining: 483ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.81s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 926.2647360\ttotal: 550ms\tremaining: 4.95s\n",
      "1:\tlearn: 906.3758016\ttotal: 976ms\tremaining: 3.9s\n",
      "2:\tlearn: 886.3932667\ttotal: 1.4s\tremaining: 3.26s\n",
      "3:\tlearn: 867.4944761\ttotal: 1.84s\tremaining: 2.76s\n",
      "4:\tlearn: 848.7492804\ttotal: 2.26s\tremaining: 2.26s\n",
      "5:\tlearn: 830.6348224\ttotal: 2.7s\tremaining: 1.8s\n",
      "6:\tlearn: 813.3819569\ttotal: 3.13s\tremaining: 1.34s\n",
      "7:\tlearn: 796.6452840\ttotal: 3.57s\tremaining: 892ms\n",
      "8:\tlearn: 780.2373449\ttotal: 3.99s\tremaining: 443ms\n",
      "9:\tlearn: 764.5699727\ttotal: 4.43s\tremaining: 0us\n",
      "Best hyper-parameters: {'n_estimators': 10} and MAE for best hyper-parameters: 563.3728300381824\n",
      "0:\tlearn: 938.9837865\ttotal: 524ms\tremaining: 4.72s\n",
      "1:\tlearn: 917.8824381\ttotal: 951ms\tremaining: 3.8s\n",
      "2:\tlearn: 897.9120165\ttotal: 1.39s\tremaining: 3.24s\n",
      "3:\tlearn: 877.7363004\ttotal: 1.85s\tremaining: 2.77s\n",
      "4:\tlearn: 858.3321187\ttotal: 2.29s\tremaining: 2.29s\n",
      "5:\tlearn: 839.9238418\ttotal: 2.73s\tremaining: 1.82s\n",
      "6:\tlearn: 821.8208251\ttotal: 3.15s\tremaining: 1.35s\n",
      "7:\tlearn: 804.6944389\ttotal: 3.58s\tremaining: 894ms\n",
      "8:\tlearn: 788.4203349\ttotal: 4.02s\tremaining: 447ms\n",
      "9:\tlearn: 771.8807408\ttotal: 4.49s\tremaining: 0us\n",
      "MAE for CatBoost final is 569.3661328661783\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning\n",
    "np.random.seed(0)\n",
    "\n",
    "cat = CatBoostRegressor()\n",
    "param_grid_cat = {'n_estimators': (1, 10)}\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "budget = 40\n",
    "\n",
    "bayes_cat = BayesSearchCV(\n",
    "        cat,\n",
    "        param_grid_cat,\n",
    "        scoring='neg_mean_absolute_error', \n",
    "        refit = False,\n",
    "        cv=partitions,\n",
    "        n_jobs=1, \n",
    "        verbose=0, \n",
    "        n_iter=budget\n",
    ")\n",
    "\n",
    "model_cat = bayes_cat.fit(Xava, yava)\n",
    "print(\"Best hyper-parameters: {} and MAE for best hyper-parameters: {}\".format(model_cat.best_params_,\n",
    "                                                                               -model_cat.best_score_))\n",
    "\n",
    "#### final model \n",
    "cat_final = CatBoostRegressor()\n",
    "cat_final.set_params(**model_cat.best_params_)\n",
    "model_cat_final = cat_final.fit(XtrainAndVal, ytrainAndVal)\n",
    "model_cat_final_test_pred = model_cat_final.predict(Xtest)\n",
    "print(\"MAE for CatBoost final is {}\".format(mae(model_cat_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should know which model performs best, and what hyper-parameters to use. Please, evaluate that best performing model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for KNN final is 511.40569863395604\n",
      "MAE for Decision Tree final is 310.9637342803766\n",
      "MAE for Random Forest final is 283.42557500000004\n",
      "MAE for Gradient Boosting final is 289.59716170662733\n",
      "MAE for CatBoost final is 569.3661328661783\n"
     ]
    }
   ],
   "source": [
    "# We have already evaluated the performances on the test set, here we only re-print the results.\n",
    "# Note that the best CatBoost has a poor performance because we trained it with a very low number of estimators.\n",
    "# This was decided because it is a slow algorithm and it took ages to perform hyper-parameter tuning in our laptops. \n",
    "#Best KNN\n",
    "print(\"MAE for KNN final is {}\".format(mae(model_knn_final_test_pred, ytest)))\n",
    "\n",
    "#Best Decision Tree\n",
    "print(\"MAE for Decision Tree final is {}\".format(mae(model_tree_final_test_pred, ytest)))\n",
    "\n",
    "#Best Random Forest\n",
    "print(\"MAE for Random Forest final is {}\".format(mae(model_rf_final_test_pred, ytest)))\n",
    "\n",
    "#Best Gradient Boosting\n",
    "print(\"MAE for Gradient Boosting final is {}\".format(mae(model_gb_final_test_pred, ytest)))\n",
    "\n",
    "# Best CatBoost\n",
    "print(\"MAE for CatBoost final is {}\".format(mae(model_cat_final_test_pred, ytest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FEATURE SELECTION WITH SELECTKBEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are going to use refit=True because this eases the way we can extract the best parameters selected by SelectKBest from the output object. We will use the datasets XtrainAndVal and ytrainAndVal so that the refitted output does not have any information from Xtest. \n",
    "\n",
    "The features selected and hyper-parameters of the Decision Tree are therefore calculated using the object *partitions*: training with Xtrain and validating with Xval (we have checked that), In addition, a final model is automatically computed with those features and hyper-parameters and we can directly make the predictions with the test set using this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'regression__min_samples_split': 63, 'regression__max_depth': 9, 'feature_selection__k': 312} and MAE for best hyper-parameters: 305.35258378511065\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "\n",
    "param_pipe_grid = {'feature_selection__k': np.arange(Xava.shape[1])+1,\n",
    "                       'regression__max_depth': np.arange(2,30),\n",
    "                       'regression__min_samples_split': np.arange(2, 100)\n",
    "}\n",
    "\n",
    "pipe_steps = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression)),\n",
    "  ('regression', tree.DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "budget = 40\n",
    "method_pipe = RandomizedSearchCV(\n",
    "    pipe_steps,\n",
    "    param_pipe_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    refit = True,\n",
    "    cv=partitions,\n",
    "    n_jobs=1,\n",
    "    verbose=0,\n",
    "    n_iter = budget\n",
    ")\n",
    "\n",
    "model_pipe = method_pipe.fit(XtrainAndVal,ytrainAndVal)\n",
    "\n",
    "print(\"Best hyper-parameters: {} and MAE for best hyper-parameters: {}\".format(model_pipe.best_params_,\n",
    "                                                                                   -model_pipe.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the results that the optimum model is using the 312 best predictor, according to the F-value of the correlation between the predictor and the output variable. We are going to predict now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Decision Tree final is 314.7982375521622\n"
     ]
    }
   ],
   "source": [
    "model_tree_pipe_final = model_pipe.best_estimator_\n",
    "\n",
    "# Train\n",
    "model_tree_pipe_final_test_pred = model_tree_pipe_final.predict(Xtest)\n",
    "print(\"MAE for Decision Tree final is {}\".format(mae(model_tree_pipe_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final performance is similar: MAE of 314.7982 vs 310.9637. However, we have not eliminated that much predictors. A possible explanation is that the function f_regression is not the one we should use. The predictor importance is calculated based on the individual correlation with the target variable, but neither interactions nor higher order dependencies are checked. We will then try with other methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FEATURE SELECTION WITH MLXTEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are going to use refit=True because this eases the way we can extract the best parameters selected by SelectKBest from the output object. We will use the datasets XtrainAndVal and ytrainAndVal so that the refitted output does not have any information from Xtest. \n",
    "\n",
    "The features selected and hyper-parameters of the Decision Tree are therefore calculated using the object *partitions*: training with Xtrain and validating with Xval (we have checked that), In addition, a final model is automatically computed with those features and hyper-parameters and we can directly make the predictions with the test set using this model. \n",
    "\n",
    "We are going to impose that it can select a model with a maximum of 50 variables. This speeds up the computation (models with more variables are slower to train) and forces simpler models.\n",
    "\n",
    "The budget is also low to speed up computation (in spite of a worse performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed: 37.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 550 out of 550 | elapsed:    8.2s finished\n",
      "Features: 1/25[Parallel(n_jobs=1)]: Done 549 out of 549 | elapsed:   13.0s finished\n",
      "Features: 2/25[Parallel(n_jobs=1)]: Done 548 out of 548 | elapsed:   17.4s finished\n",
      "Features: 3/25[Parallel(n_jobs=1)]: Done 547 out of 547 | elapsed:   22.2s finished\n",
      "Features: 4/25[Parallel(n_jobs=1)]: Done 546 out of 546 | elapsed:   26.7s finished\n",
      "Features: 5/25[Parallel(n_jobs=1)]: Done 545 out of 545 | elapsed:   24.6s finished\n",
      "Features: 6/25[Parallel(n_jobs=1)]: Done 544 out of 544 | elapsed:   21.4s finished\n",
      "Features: 7/25[Parallel(n_jobs=1)]: Done 543 out of 543 | elapsed:   39.0s finished\n",
      "Features: 8/25[Parallel(n_jobs=1)]: Done 542 out of 542 | elapsed:   34.3s finished\n",
      "Features: 9/25[Parallel(n_jobs=1)]: Done 541 out of 541 | elapsed:   42.5s finished\n",
      "Features: 10/25[Parallel(n_jobs=1)]: Done 540 out of 540 | elapsed:   51.5s finished\n",
      "Features: 11/25[Parallel(n_jobs=1)]: Done 539 out of 539 | elapsed:   55.6s finished\n",
      "Features: 12/25[Parallel(n_jobs=1)]: Done 538 out of 538 | elapsed:   59.5s finished\n",
      "Features: 13/25[Parallel(n_jobs=1)]: Done 537 out of 537 | elapsed:   57.3s finished\n",
      "Features: 14/25[Parallel(n_jobs=1)]: Done 536 out of 536 | elapsed:   53.6s finished\n",
      "Features: 15/25[Parallel(n_jobs=1)]: Done 535 out of 535 | elapsed:  1.1min finished\n",
      "Features: 16/25[Parallel(n_jobs=1)]: Done 534 out of 534 | elapsed:  1.2min finished\n",
      "Features: 17/25[Parallel(n_jobs=1)]: Done 533 out of 533 | elapsed:  1.3min finished\n",
      "Features: 18/25[Parallel(n_jobs=1)]: Done 532 out of 532 | elapsed:  1.4min finished\n",
      "Features: 19/25[Parallel(n_jobs=1)]: Done 531 out of 531 | elapsed:  1.5min finished\n",
      "Features: 20/25[Parallel(n_jobs=1)]: Done 530 out of 530 | elapsed:  1.5min finished\n",
      "Features: 21/25[Parallel(n_jobs=1)]: Done 529 out of 529 | elapsed:  1.6min finished\n",
      "Features: 22/25[Parallel(n_jobs=1)]: Done 528 out of 528 | elapsed:  1.6min finished\n",
      "Features: 23/25[Parallel(n_jobs=1)]: Done 527 out of 527 | elapsed:  1.7min finished\n",
      "Features: 24/25[Parallel(n_jobs=1)]: Done 526 out of 526 | elapsed:  1.8min finished\n",
      "Features: 25/25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'sfs__k_features': 25, 'sfs__estimator__min_samples_split': 32, 'sfs__estimator__max_depth': 24} and MAE for best hyper-parameters: 365.7979599692071\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "\n",
    "\n",
    "sfs_tree = SFS(tree.DecisionTreeRegressor(), \n",
    "           k_features=1,\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='neg_mean_absolute_error',\n",
    "           cv=2, verbose = 1)\n",
    "'''\n",
    "model_sfs_tree = sfs_tree.fit(XtrainAndVal,ytrainAndVal)\n",
    "print(\"Best attribute subset: {} and MAE for best attribute subset: {}\".format(model_sfs_tree.k_feature_names_,\n",
    "                                                                                   -model_sfs_tree.k_score_))\n",
    "'''\n",
    "sfs_pipe = Pipeline([('sfs', sfs_tree),\n",
    "                     ('regression', tree.DecisionTreeRegressor())])\n",
    "\n",
    "param_sfs_pipe_grid = {'sfs__k_features': (1, 25),\n",
    "                       'sfs__estimator__max_depth': np.arange(2,30),\n",
    "                       'sfs__estimator__min_samples_split': np.arange(2, 100)}\n",
    "\n",
    "budget = 10\n",
    "method_sfs_pipe = RandomizedSearchCV(sfs_pipe,\n",
    "                                     param_sfs_pipe_grid,\n",
    "                                     scoring='neg_mean_absolute_error',\n",
    "                                     n_jobs=2,\n",
    "                                     cv=partitions,\n",
    "                                     refit=True,\n",
    "                                     verbose=1,\n",
    "                                     n_iter=budget\n",
    "                                    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model_sfs_pipe = method_sfs_pipe.fit(XtrainAndVal,ytrainAndVal)\n",
    "\n",
    "print(\"Best hyper-parameters: {} and MAE for best hyper-parameters: {}\".format(model_sfs_pipe.best_params_,\n",
    "                                                                                   -model_sfs_pipe.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Decision Tree final is 367.2505829383886\n"
     ]
    }
   ],
   "source": [
    "model_sfs_pipe_final = model_sfs_pipe.best_estimator_\n",
    "\n",
    "# Train\n",
    "model_sfs_pipe_final = model_sfs_pipe_final.predict(Xtest)\n",
    "print(\"MAE for Decision Tree final is {}\".format(mae(model_sfs_pipe_final, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final error is higher than expected, probably because we have restricted the budget (and the number of attributes; note that the model has selected 25 attributes, the maximum!) to decrease the computational time and a better hyper-parameter configuration was not found.\n",
    "\n",
    "We could also try backward sequential feature selection, instead of forward and check how we obtain a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. USE ONLY PREDICTORS FROM EXACT SOTAVENTO LOCATION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create new predictor (X) matrices only with Sotavento predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "Xdata_sotav = data.iloc[:,18:550:25]\n",
    "\n",
    "Xsotav = Xdata_sotav.as_matrix()\n",
    "\n",
    "XtrainAndVal_sotav = Xsotav[indicesTrainAndVal,:]\n",
    "Xtest_sotav = Xsotav[indicesTest,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this dataset train a hyper-parameter-tuned Decision Tree and test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\toni3\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'max_depth': 9, 'min_samples_split': 96} and MAE for best hyper-parameters: 302.7641351682713\n",
      "MAE for Sotavento Predictors is 316.9406557818423\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tree_sotav = tree.DecisionTreeRegressor()\n",
    "param_grid_tree_sotav = {'max_depth': (2,30), \n",
    "                   'min_samples_split': (2,100)                \n",
    "}\n",
    "\n",
    "budget = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bayes_tree_sotav = BayesSearchCV(\n",
    "            tree_sotav, \n",
    "            param_grid_tree_sotav, \n",
    "            scoring='neg_mean_absolute_error', \n",
    "            refit = False,\n",
    "            cv=partitions, \n",
    "            n_jobs=4, \n",
    "            verbose=0, \n",
    "            n_iter=budget\n",
    "    )\n",
    "\n",
    "    sotav = bayes_tree_sotav.fit(Xsotav, yava)\n",
    "    print(\"Best hyper-parameters: {} and MAE for best hyper-parameters: {}\".format(sotav.best_params_, -sotav.best_score_))\n",
    "    \n",
    "    sotav_final = tree.DecisionTreeRegressor()\n",
    "    sotav_final.set_params(**sotav.best_params_)\n",
    "    model_sotav_final = sotav_final.fit(XtrainAndVal_sotav, ytrainAndVal)\n",
    "    model_sotav_final_test_pred = model_sotav_final.predict(Xtest_sotav)\n",
    "    print(\"MAE for Sotavento Predictors is {}\".format(mae(model_sotav_final_test_pred, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is quite similar to the performance obtained with all the predictors: Mae of 316.9407 vs 310.9637. And we are using much less attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. USE RANDOM FOREST PREDICTOR IMPORTANCE (EXTRA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest computes the predictor importance from the Out-Of-Bag estimation of performance of the different sub-trees of the Random Forest. \n",
    "\n",
    "In Random Forest, when training the individual estimators (in our case, the small scikit Decision Trees), for each estimator, there are instances not used in the training. This is because each of the estimators is trained with a random subset of the entire dataset. Then, Random Forest can compute \"Out-Of-Bag\" errors in each estimator. That is, a Decision Tree is trained with a random subset of the dataset. With the instances not included, it makes predictions and calculates the error (we call them Out-of-bag errors). This is done for each individual estimator of the Random Forest. So, finally, an average error is computed. \n",
    "\n",
    "And these Out-Of-Bag errors are used to estimate the predictor importance! What the algorithm does internally is, first, to calculate each Out-Of-Bag error in the usual way. Then, it shuffles one of the attributes, and re-compute the error. If the error difference is small, we can say that that attribute is not very important. Because even putting random numbers in that attribute, the error is not affected. On the other hand, if the error increases a lot, the attribute is considered very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model_rf_final.feature_importances_\n",
    "\n",
    "# Compute standard deviations\n",
    "std = np.std([tree.feature_importances_ for tree in model_rf_final.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "#print(\"Feature ranking:\")\n",
    "\n",
    "#for f in range(XtrainAndVal.shape[1]):\n",
    "    #print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(XtrainAndVal.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(XtrainAndVal.shape[1]), indices)\n",
    "plt.xlim([-1, XtrainAndVal.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of attributes to keep we will use the Elbow Method (for simplicity, we ignore the standard deviations). Is it possible to set the number of attributes as an hyper-parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We should keep 29 attributes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = importances[indices]\n",
    "x = range(1, len(y) + 1)\n",
    "kn = KneeLocator(x, y, curve = 'convex', direction = 'decreasing')\n",
    "\n",
    "plt.xlabel('Number of attributes')\n",
    "plt.ylabel('Attribute Importance')\n",
    "plt.plot(x, y, 'bx-')\n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "print(\"We should keep %d attributes\" % kn.knee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the first 29 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xava_rf = Xava[:,indices]\n",
    "Xava_rf = Xava_rf[:, 0:28]\n",
    "\n",
    "Xtest_rf = Xtest[:,indices]\n",
    "Xtest_rf = Xtest_rf[:,0:28]\n",
    "\n",
    "XtrainAndVal_rf = XtrainAndVal[:, indices]\n",
    "XtrainAndVal_rf = XtrainAndVal_rf[:, 0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bayes search max_depth, min_samples_split: {'max_depth': 29, 'min_samples_split': 76} and MAE for best Decision tree: 306.2005894012507\n",
      "MAE for Decision Tree final is 316.32129495737126\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning --> Bayesian Optimization\n",
    "np.random.seed(0)\n",
    "\n",
    "#In spite of the different hyper-parameters for a decision tree (max_depth, min_samples_split, max_features, min_samples_leaf),\n",
    "#only max_depth and min_samples_split are used because of the academic goal of this study\n",
    "param_tree_bayes_grid = {'max_depth': (2,30),\n",
    "                         'min_samples_split': (2,100)\n",
    "}\n",
    "budget = 100\n",
    "method_tree_bayes_att = BayesSearchCV(\n",
    "        method_tree, \n",
    "        param_tree_bayes_grid, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        refit = False,\n",
    "        cv = partitions, \n",
    "        n_jobs =1, \n",
    "        verbose = 0,\n",
    "        n_iter = budget\n",
    ")\n",
    "\n",
    "model_tree_bayes_att = method_tree_bayes_att.fit(Xava_rf, yava)\n",
    "print(\"Best Bayes search max_depth, min_samples_split: {} \\\n",
    "and MAE for best Decision tree: {}\".format(model_tree_bayes_att.best_params_, -model_tree_bayes_att.best_score_))\n",
    "\n",
    "tree_final_att = tree.DecisionTreeRegressor()\n",
    "tree_final_att.set_params(**model_tree_bayes_att.best_params_)\n",
    "model_tree_final_att = tree_final_att.fit(XtrainAndVal_rf, ytrainAndVal)\n",
    "model_tree_final_att_test_pred = model_tree_final_att.predict(Xtest_rf)\n",
    "print(\"MAE for Decision Tree final is {}\".format(mae(model_tree_final_att_test_pred, ytest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is similar (316.3213 vs 310.9637) and we are using 5% of the attributes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
